{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35cc446-95e3-489a-8079-43519f8b5092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/common_code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef28ca78-71e9-4ce6-a137-5eed85df6058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestion circuits.csv\n",
    "1. Read circuits.csv file from raw container using Dataframe reader API\n",
    "2. Select only the required columns or drop the unwanted columns\n",
    "3. Rename few columns to the suitable name\n",
    "4. Add audit column which specifies the ingestion date/time/timestamp\n",
    "5. Write the final transformed data into processsed container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c873a4ed-8d9d-4bfc-9d93-9c3166049500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# listing all the mounts\n",
    "display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94fba9f3-1c67-4a0e-a71d-82da6711f019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# listing all the files in raw container\n",
    "# display(dbutils.fs.ls(\"/mnt/formula1dlsiddhi/raw\"))\n",
    "display(dbutils.fs.ls(raw_container_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62cbb58b-a3f6-4ad8-90d3-f4c27f0be90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Visit [spark_documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.read_csv.html#pyspark.pandas.read_csv) for read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f72af4-18ef-4a5c-83ae-c35a0a987895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ac222d-c161-47cd-b093-daf4375f876c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Defining the schema manually\n",
    "1. Visit [spark documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructType.html) for Struct_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8e01e0-5803-4ead-88c0-52424720bae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_schema = StructType([ StructField(\"circuitId\", IntegerType(), True),\n",
    "                               StructField(\"circuitRef\", StringType(), True),\n",
    "                               StructField(\"name\", StringType(), True),\n",
    "                               StructField(\"location\", StringType(), True),\n",
    "                               StructField(\"country\", StringType(), True),\n",
    "                               StructField(\"lat\", DoubleType(), True),\n",
    "                               StructField(\"lng\", DoubleType(), True),\n",
    "                               StructField(\"alt\", IntegerType(), True),\n",
    "                               StructField(\"url\", StringType(), True)\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40364350-e993-4ffd-923c-8643fce24125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df = spark.read.schema(circuits_schema) \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .csv(f\"{raw_container_path}/circuits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d80f4b-5c93-4400-b911-9f0de0ccd900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# printing schema\n",
    "circuits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b14896-bb1c-4bfe-99bd-f5de09bed5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(circuits_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190f5e8e-8015-4261-95f8-de1fc7ad9ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Further processing 1\n",
    "1. From the above dataframe we don't need URL, so either select only the required columns or drop the unwanted column url\n",
    "2. Rename the following columns\n",
    "  - circuitId ---> circuit_id\n",
    "  - circuitRef ---> circuit_ref\n",
    "  - name ---> circuit_name\n",
    "  - location ---> circuit_location\n",
    "  - country ---> circuit_country\n",
    "  - lat ---> circuit_latitude\n",
    "  - long ---> circuit_longitude\n",
    "  - alt ---> circuit_altitude\n",
    "3. Add the audit column ingestion_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a380cdf5-9d13-47e9-a250-ab1c77578683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df6c40d-ea3f-4f87-b17c-b6cb41a7cbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# you can col, dataframe.col() to select columns here I used the simple method\n",
    "# Can be used like this as well\n",
    "# circuits_df_selected = circuits_df.select(col(\"circuitId\").alias(\"circuit_id\"), col(\"circuitRef\").alias(\"circuit_ref\"), col(\"name\").alias(\"circuit_name\"), col(\"location\").alias(\"circuit_location\"), col(\"country\").alias(\"circuit_country\"), col(\"lat\").alias(\"circuit_lat\"), col(\"lng\").alias(\"circuit_lng\"), col(\"alt\").alias(\"circuit_alt\"))\n",
    "# Can be used like this as well\n",
    "# circuits_df_selected = circuits_df.select(circuits_df.circuitId.alias(\"circuit_id\"), ......)\n",
    "circuits_df_selected = circuits_df.select(\"circuitId\", \"circuitRef\", \"name\", \"location\", \"country\", \"lat\", \"lng\", \"alt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0ebf4f-2fe2-4e7c-9bae-72d3cc62e401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(circuits_df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b0f150-83fe-437c-a882-e1f86d279f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df_columns_renamed = circuits_df_selected.withColumnRenamed(\"circuitId\", \"circuit_id\") \\\n",
    "                                                  .withColumnRenamed(\"circuitRef\", \"circuit_ref\") \\\n",
    "                                                  .withColumnRenamed(\"name\", \"circuit_name\") \\\n",
    "                                                  .withColumnRenamed(\"location\", \"circuit_location\") \\\n",
    "                                                  .withColumnRenamed(\"country\", \"circuit_country\") \\\n",
    "                                                  .withColumnRenamed(\"lat\", \"circuit_lat\") \\\n",
    "                                                  .withColumnRenamed(\"lng\", \"circuit_lng\") \\\n",
    "                                                  .withColumnRenamed(\"alt\", \"circuit_altitude\")\n",
    "display(circuits_df_columns_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af836a9d-2b91-42f3-8681-6754bb1d54de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a66306-9782-4788-8035-e810f0d048b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df_add_audit_column = ingest_current_timestamp(circuits_df_columns_renamed)\n",
    "display(circuits_df_add_audit_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f10b6105-814c-44f8-ad7c-99c58df26e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For naming conventions copy the circuits_df_add_audit_column DataFrame to a new DataFrame with the name circuits_df_final\n",
    "circuits_df_final = circuits_df_add_audit_column\n",
    "display(circuits_df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abec69ea-690a-47db-8aa8-61dbfae66f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Write the final data into processed folder\n",
    "1. Visit [spark documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.write.html?highlight=write) for write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f24f2d86-56b3-4768-a972-fabf87c33359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df_final.write.mode(\"overwrite\") \\\n",
    "                       .parquet(f\"{processed_container_path}/circuits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7a8c97-4819-40bc-8735-a2fd60cb9ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify that the data was written to the parquet file\n",
    "df = spark.read.parquet(f\"{processed_container_path}/circuits\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b508b35-9d80-460f-92fc-3720271f8d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"success...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049a82ad-0613-49d3-add1-25e73eb8d7db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Data_ingestion_circuits_csv_file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}